{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fdffe2",
   "metadata": {},
   "source": [
    "<img src=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647c2bc",
   "metadata": {},
   "source": [
    "# Project Goal \n",
    "> - To predict what percentage of a Teams overall budget will be paid to the quarterback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb38285",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "> - Using data aquired from various websites we ran correlation tests to find the most statistically significant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2d6ca",
   "metadata": {},
   "source": [
    "# Initial Hypothesis\n",
    "> - Players who extend the season of their team I.E playoffs will have a higher percentage of their teams salary cap\n",
    "> - Players who have more yards and touchdowns will have a higher percentage of their teams salary cap\n",
    "> - Players who have more interceptions will have a lower percentage of their teams salary cap\n",
    "> - Players who have a higher passer rating will have a higher percentage of their teams salary cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2eb1a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "import wrangle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression, SequentialFeatureSelector\n",
    "from pydataset import data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e276a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('last_csv.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c915e9e",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "> * Data acquired from(web scrape):\n",
    "    - https://overthecap.com/position/quarterback\n",
    "    - https://overthecap.com/contract-history/quarterback\n",
    "        - Original player salary data consisted of 1,590 and 11 columns. Each row contained a player and their salary for a given year.\n",
    "    - https://www.nfl.com/schedules/2021/POST4/\n",
    "        - Original playoff data consisted of 162 rows and 7 columns. Each row contained a team and the round that team made it to in a given year between 2010 and 2022.\n",
    "    - https://www.pro-football-reference.com/years/2022/passing.htm\n",
    "        - Original player stats data set consisted of 458 rows and 19 columns. Each row contained a single player for a given year between 2010 and 2022.\n",
    "> * Cached all files to local csv\n",
    "> * Each row player stats throughout a specific year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3764a",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "> * Visualized full dataset for univariate exploration\n",
    "      * Histograms different types of distributions\n",
    "\n",
    "> * Verified datatypes\n",
    "> * Corrected column names\n",
    "> * Checked for nulls and removed them\n",
    "> * Once the 3 data sets were merged, we cleaned the data and conducted feature engineering. Our final data frame consisted of 410 rows and 42 columns for exploration. Each row contains a single player with their stats for a respective year from 2010 to 2022. \n",
    "> * Cached combined files to local csv\n",
    "\n",
    "> * Split the data, stratifying on target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1d57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940cc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wrangle.split_data(df)\n",
    "columns_list, target, corr_test = wrangle.get_target_and_columns(df, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287477b",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c39fc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrangle.new_visual_univariate_findings(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2d22fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrangle.univariate_findings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4fa41",
   "metadata": {},
   "source": [
    "# Univariate Exploration Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17366ac",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60239c2b",
   "metadata": {},
   "source": [
    "$H_0$: There is no correlation between our selected features and our target variable.\n",
    "\n",
    "$H_\\alpha$: There is a correlation between our selected features and our target variable.\n",
    "\n",
    "$\\alpha$: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a7f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrangle.new_visual_multivariate_findings(df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56688ff9",
   "metadata": {},
   "source": [
    "# Bivariate Exploration Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51322aeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrangle.get_explore_data(columns_list, corr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfbdc3",
   "metadata": {},
   "source": [
    "# Correlation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d058126",
   "metadata": {},
   "source": [
    "> - We will use a confidence interval of 95%\n",
    "> - the resulting alpha is .05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc8c95",
   "metadata": {},
   "source": [
    "$H_0$: There is no statistical significance between our selected features and our target variable.\n",
    "\n",
    "$H_\\alpha$: There is a statistical significance between our selected features and our target variable.\n",
    "\n",
    "$\\alpha$: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc5889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_test.sort_values(by= 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = corr_test.feature[corr_test.p < .05].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88396",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f3216",
   "metadata": {},
   "source": [
    "- We will use RMSE as our evaluation metric\n",
    "\n",
    "** by using baseline as an evaluation metric we can be accurate to within 6.9 <br>\n",
    "** 6.9 will be the baseline RMSE we will use for this project <br>\n",
    "<br>\n",
    "** I will be evaluating models developed using four different model types and various hyperparameter configurations * Models will be evaluated on train and validate data * The model that performs the best will then be evaluated on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995e86c",
   "metadata": {},
   "source": [
    "## Features we are moving forward with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_test[corr_test.p < .05].sort_values(by='p').reset_index().drop(columns ='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data in its respective catagory\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = wrangle.get_X_train_val_test(train,validate, test, columns_list,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling on selected features to be sent into model\n",
    "X_train, X_validate, X_test = wrangle.scale_data(X_train, X_validate,X_test,cols = columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the data through the models\n",
    "df1, df2, df3,predict_linear, feature_weights, predict_linear_test  = wrangle.get_model_numbers(X_train, X_validate, X_test, y_train, y_validate, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86acd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predicted'] = predict_linear.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c036d",
   "metadata": {},
   "source": [
    "# Looking at predicted vs actual for a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c829214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "train[['predicted','percent_of_cap', 'year']].sort_values(by=['percent_of_cap','year'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90242e7a",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models on the training data\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24aedc1",
   "metadata": {},
   "source": [
    "## Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa2041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Models on the validate data\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db40092",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea55c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model on the unseen test data\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca57807",
   "metadata": {},
   "source": [
    "# Using fold method for splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb078d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the data through the models with the new train, test method.\n",
    "master_df,best_parameters = wrangle.run_fold(df, columns_list, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a469157",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the test data through the models\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c8845",
   "metadata": {},
   "source": [
    "# Modeling Summary\n",
    "> - Our ordinary least squared(OLS) performed best with an RMSE score of 5.479861e+00 in validate\n",
    "> - Our unseen test data beat baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528c3be",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "> - The different columns were distributed in differently see above for a chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373fcc8",
   "metadata": {},
   "source": [
    "### Features that were statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b460005",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_test[corr_test.p < .05].sort_values(by='p').reset_index().drop(columns ='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a1d35",
   "metadata": {},
   "source": [
    "### NLP Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import w_wrangle as wran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire and Prepare\n",
    "\n",
    "comm = wran.acquire_commentary()\n",
    "comm.player_commentary = comm.player_commentary.apply(wran.clean_strings)\n",
    "\n",
    "comm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get grams to put into visualizations\n",
    "\n",
    "unigram_high_words, unigram_mid_words, unigram_low_words, bi_tri_high_words, bi_tri_mid_words, bi_tri_low_words = wran.get_grams(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734adc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigram visualizations\n",
    "\n",
    "wran.viz_unigrams(unigram_high_words, unigram_mid_words, unigram_low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9516fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram visualizations\n",
    "\n",
    "wran.viz_bigrams(bi_tri_high_words, bi_tri_mid_words, bi_tri_low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigram visualizations\n",
    "\n",
    "wran.viz_trigrams(bi_tri_high_words, bi_tri_mid_words, bi_tri_low_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856487c",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Sentiment Scores\n",
    "\n",
    "wran.get_sia_scores(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd9624",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "- 'Career' is not mentioned in low-percentage caps.\n",
    "- High-percentage caps have a larger set of unique words. A lot of them could be comparisons to \"the Greats\" (Mahomes, Hurts, Wentz, Brady).\n",
    "- Low-percentage caps talked a lot about backup (presumably quarterbacks), field, and run. Mention of \"Jets\" indicate historically low-percentage cap quarterbacks.\n",
    "- The conversation always revolves around winning the Super Bowl across all three cap tiers.\n",
    "- High-percentage caps talk about the Super Bowl and their performance in it significantly more. The focus of mid and low percentage caps also speak about their performance in the NFC.\n",
    "- Sentiment scores are very high, if not maxed out, across all tiers. Only the low-percentage cap quarterbacks had a slightly lower score.\n",
    "- Rushing attempts for Quarterbacks is trending up, which indicates an evolution for the position. There is a major spike after 2020.\n",
    "- Quarterbacks who make the playoffs demand a higher percentage of a team's salary cap regardless of the round their team makes it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f609a7",
   "metadata": {},
   "source": [
    "# Recommendations\n",
    "> - Because our model was able to beat baseline we recommend using our model to assess value to Quarterbacks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b66c5",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "> - Look into retrieving corpus from a text aggregator like ChatGPT.\n",
    "> - Run throught the entire pipeline with different positons other than just quarterback\n",
    "> - Look into quantifying qualitative attributes of players. Behavioral interviews, Wonderlic tests, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
